{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#LogisticRegression\" data-toc-modified-id=\"LogisticRegression-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>LogisticRegression</a></span></li><li><span><a href=\"#DecisionTreeClassifier\" data-toc-modified-id=\"DecisionTreeClassifier-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>DecisionTreeClassifier</a></span></li><li><span><a href=\"#RandomForestClassifier\" data-toc-modified-id=\"RandomForestClassifier-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>RandomForestClassifier</a></span></li><li><span><a href=\"#LGBMClassifier\" data-toc-modified-id=\"LGBMClassifier-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>LGBMClassifier</a></span></li><li><span><a href=\"#XGBClassifier\" data-toc-modified-id=\"XGBClassifier-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>XGBClassifier</a></span></li></ul></li><li><span><a href=\"#Тестирование-модели\" data-toc-modified-id=\"Тестирование-модели-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Тестирование модели</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to create process using 'C:\\ProgramData\\Anaconda3\\envs\\ds_practicum_env\\python.exe C:\\ProgramData\\Anaconda3\\envs\\ds_practicum_env\\Scripts\\pip-script.py install -q lightgbm'\n"
     ]
    }
   ],
   "source": [
    "!pip install -q lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to create process using 'C:\\ProgramData\\Anaconda3\\envs\\ds_practicum_env\\python.exe C:\\ProgramData\\Anaconda3\\envs\\ds_practicum_env\\Scripts\\pip-script.py install -q xgboost'\n"
     ]
    }
   ],
   "source": [
    "!pip install -q xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to create process using 'C:\\ProgramData\\Anaconda3\\envs\\ds_practicum_env\\python.exe C:\\ProgramData\\Anaconda3\\envs\\ds_practicum_env\\Scripts\\pip-script.py install -q Pipeline'\n"
     ]
    }
   ],
   "source": [
    "!pip install -q Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to create process using 'C:\\ProgramData\\Anaconda3\\envs\\ds_practicum_env\\python.exe C:\\ProgramData\\Anaconda3\\envs\\ds_practicum_env\\Scripts\\pip-script.py install -q nltk'\n"
     ]
    }
   ],
   "source": [
    "!pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# импортируем все необходимое\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "#from pymystem3 import Mystem\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# отключаем предупреждения\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('toxic_comments.csv')\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df['toxic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.841344371679229"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ratio = df['toxic'].value_counts()[0] / df['toxic'].value_counts()[1]\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы не сбалансированны. Учтем это при обучении моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упростим текст для извлечения признаков.\n",
    "\n",
    "Этапы предобработки текста:\n",
    "\n",
    "Токенизация (англ. tokenization) — разбиение текста на токены: отдельные фразы, слова, символы.\n",
    "Лемматизация (англ. lemmatization) — приведение слова к начальной форме (лемме).\n",
    "\n",
    "Напишем функцию для токенизации и лемматизации текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemm_text(text):\n",
    "    text = re.sub(r'[^a-zA-z ]', ' ', text)\n",
    "    text = text.lower()\n",
    "    token = nltk.word_tokenize(text)\n",
    "    text = [word for word in token if word not in stop_words]\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>aww match background colour seemingly stuck th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  explanation edits made username hardcore metal...      0\n",
       "1           1  aww match background colour seemingly stuck th...      0\n",
       "2           2  hey man really trying edit war guy constantly ...      0\n",
       "3           3  make real suggestion improvement wondered sect...      0\n",
       "4           4                      sir hero chance remember page      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: данные обработаны, очищены от лишних символов, лемматизированы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = np.random.RandomState(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('toxic', axis=1)\n",
    "y = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.1, random_state = RANDOM_STATE, stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (143362, 2) (143362,)\n"
     ]
    }
   ],
   "source": [
    "print('Train data shape:', X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (15930, 2) (15930,)\n"
     ]
    }
   ],
   "source": [
    "print('Test data shape:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для поиска лучших гиперпараметров используя RandomizedSearchCV с кросс-валидацией на 5-ти подвыборках и расчетом TF-IDF для каждой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, params):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(min_df = 1)),\n",
    "        ('model', model)])\n",
    "    grid = RandomizedSearchCV(estimator = pipeline, param_distributions = params,\n",
    "                              cv = 5, scoring = 'f1',\n",
    "                              n_jobs = -1, refit = True,\n",
    "                             random_state = RANDOM_STATE)\n",
    "    grid.fit(X_train['text'], y_train)\n",
    "    print('Лучший результат F1:', grid.best_score_)\n",
    "    print('Лучшие параметры модели:', grid.best_params_)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший результат F1: 0.7675270409603879\n",
      "Лучшие параметры модели: {'model__penalty': 'l2', 'model__C': 10.0}\n"
     ]
    }
   ],
   "source": [
    "lr = train_model(LogisticRegression(random_state = RANDOM_STATE, class_weight = 'balanced'), {\n",
    "    'model__C': [0.1, 1.0, 10.0],\n",
    "    'model__penalty': ['l2']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший результат F1: 0.5028559437500981\n",
      "Лучшие параметры модели: {'model__max_depth': 6, 'model__criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "dt = train_model(DecisionTreeClassifier(random_state = RANDOM_STATE, class_weight = 'balanced'), {\n",
    "    'model__criterion':['gini','entropy'],\n",
    "    'model__max_depth':[2,4,6]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший результат F1: 0.2875242298979237\n",
      "Лучшие параметры модели: {'model__max_features': 19, 'model__max_depth': 15}\n"
     ]
    }
   ],
   "source": [
    "rf = train_model(RandomForestClassifier(random_state = RANDOM_STATE, class_weight = 'balanced'), {\n",
    "    'model__max_depth':[5,15],\n",
    "    'model__max_features' : list(range(1,20, 2))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший результат F1: 0.7576468883353134\n",
      "Лучшие параметры модели: {'model__max_depth': 15, 'model__learning_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "lgbm = train_model(\n",
    "    lgb.LGBMClassifier(random_state = RANDOM_STATE, class_weight = 'balanced'), {\n",
    "    'model__max_depth':[2,10,15],\n",
    "    'model__learning_rate': [0.1, 0.3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:04:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"class_weight\" } are not used.\n",
      "\n",
      "Лучший результат F1: 0.7550916350588797\n",
      "Лучшие параметры модели: {'model__learning_rate': 0.5}\n"
     ]
    }
   ],
   "source": [
    "xgb = train_model(\n",
    "    xgb.XGBClassifier(random_state = RANDOM_STATE, booster = 'gbtree', class_weight = 'balanced'), {\n",
    "        'model__learning_rate': [0.5, 1.0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "    \n",
    "    Лучший результат на тренировочной выборке показала модель LogisticRegression:\n",
    "        \n",
    "        F1: 0.7675270409603879\n",
    "        \n",
    "        Лучшие параметры модели: {'model__penalty': 'l2', 'model__C': 10.0}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем такую же функцию для обучения и расчета F1 на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший результат F1: 0.758004038073262\n"
     ]
    }
   ],
   "source": [
    "# Создаем объект CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Преобразуем текстовые данные в числовые\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train['text'])\n",
    "X_test_vectorized = vectorizer.transform(X_test['text'])\n",
    "\n",
    "# Обучаем модель\n",
    "lr_test = LogisticRegression(penalty='l2', C=10.0, random_state=RANDOM_STATE, class_weight='balanced')\n",
    "lr_test.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Делаем предсказания на тестовом наборе данных\n",
    "y_pred = lr_test.predict(X_test_vectorized)\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Лучший результат F1:', f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 лучшей модели LogisticRegression на тестовой выборке показал резульат 0.758. Уровень метрики соответствует требованиям, значит цель достугнута."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Мы получили набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Подготовили данные. Выполнили:\n",
    "\n",
    "Токенизацию (англ. tokenization) — разбиение текста на токены: отдельные фразы, слова, символы.\n",
    "\n",
    "Лемматизацию (англ. lemmatization) — приведение слова к начальной форме (лемме).\n",
    "\n",
    "2. Обучили 5 моделей классификации.\n",
    "\n",
    "Лучший результат на тренировочной выборке показала модель LogisticRegression:\n",
    "\n",
    "    F1: 0.7675270409603879\n",
    "\n",
    "    Лучшие параметры модели: {'model__penalty': 'l2', 'model__C': 10.0}\n",
    "    \n",
    "3. Уровень метрики F1 для лучшей модели LogisticRegression на тестовой выборке показал резульат 0.758."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3054,
    "start_time": "2023-06-02T06:44:05.443Z"
   },
   {
    "duration": 2060,
    "start_time": "2023-06-02T06:44:08.500Z"
   },
   {
    "duration": 2653,
    "start_time": "2023-06-02T06:44:10.562Z"
   },
   {
    "duration": 2077,
    "start_time": "2023-06-02T06:44:13.218Z"
   },
   {
    "duration": 1931,
    "start_time": "2023-06-02T06:44:15.297Z"
   },
   {
    "duration": 241,
    "start_time": "2023-06-02T06:44:17.230Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-02T06:44:17.473Z"
   },
   {
    "duration": 2066,
    "start_time": "2023-06-02T06:44:17.481Z"
   },
   {
    "duration": 49,
    "start_time": "2023-06-02T06:44:19.549Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-02T06:44:19.600Z"
   },
   {
    "duration": 22,
    "start_time": "2023-06-02T06:44:19.608Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-02T06:44:19.632Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-02T06:44:19.637Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-02T06:44:19.645Z"
   },
   {
    "duration": 75117,
    "start_time": "2023-06-02T06:44:19.655Z"
   },
   {
    "duration": 23,
    "start_time": "2023-06-02T06:45:34.777Z"
   },
   {
    "duration": 62812,
    "start_time": "2023-06-02T06:51:26.859Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-02T06:52:29.674Z"
   },
   {
    "duration": 3011,
    "start_time": "2023-06-02T06:59:33.960Z"
   },
   {
    "duration": 2101,
    "start_time": "2023-06-02T06:59:36.974Z"
   },
   {
    "duration": 2095,
    "start_time": "2023-06-02T06:59:39.079Z"
   },
   {
    "duration": 2042,
    "start_time": "2023-06-02T06:59:41.178Z"
   },
   {
    "duration": 1950,
    "start_time": "2023-06-02T06:59:43.223Z"
   },
   {
    "duration": 178,
    "start_time": "2023-06-02T06:59:45.175Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-02T06:59:45.355Z"
   },
   {
    "duration": 1851,
    "start_time": "2023-06-02T06:59:45.362Z"
   },
   {
    "duration": 33,
    "start_time": "2023-06-02T06:59:47.214Z"
   },
   {
    "duration": 29,
    "start_time": "2023-06-02T06:59:47.250Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-02T06:59:47.281Z"
   },
   {
    "duration": 2,
    "start_time": "2023-06-02T06:59:47.290Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-02T06:59:47.294Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-02T06:59:47.301Z"
   },
   {
    "duration": 67931,
    "start_time": "2023-06-02T06:59:47.306Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-02T07:00:55.239Z"
   },
   {
    "duration": 87,
    "start_time": "2023-06-02T07:03:59.782Z"
   },
   {
    "duration": 87,
    "start_time": "2023-06-02T07:07:01.482Z"
   },
   {
    "duration": 100,
    "start_time": "2023-06-02T07:07:43.946Z"
   },
   {
    "duration": 2668,
    "start_time": "2023-06-02T07:08:14.982Z"
   },
   {
    "duration": 2123,
    "start_time": "2023-06-02T07:08:17.652Z"
   },
   {
    "duration": 2715,
    "start_time": "2023-06-02T07:08:19.778Z"
   },
   {
    "duration": 2714,
    "start_time": "2023-06-02T07:08:22.496Z"
   },
   {
    "duration": 1897,
    "start_time": "2023-06-02T07:08:25.213Z"
   },
   {
    "duration": 229,
    "start_time": "2023-06-02T07:08:27.112Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-02T07:08:27.343Z"
   },
   {
    "duration": 2110,
    "start_time": "2023-06-02T07:08:27.351Z"
   },
   {
    "duration": 53,
    "start_time": "2023-06-02T07:08:29.463Z"
   },
   {
    "duration": 9,
    "start_time": "2023-06-02T07:08:29.521Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-02T07:08:29.532Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-02T07:08:29.548Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-02T07:08:29.577Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-02T07:08:29.582Z"
   },
   {
    "duration": 80879,
    "start_time": "2023-06-02T07:08:29.590Z"
   },
   {
    "duration": 15,
    "start_time": "2023-06-02T07:09:50.471Z"
   },
   {
    "duration": 32,
    "start_time": "2023-06-02T07:09:50.488Z"
   },
   {
    "duration": 22,
    "start_time": "2023-06-02T07:09:50.523Z"
   },
   {
    "duration": 109,
    "start_time": "2023-06-02T07:09:50.548Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-02T07:09:50.660Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-02T07:09:50.678Z"
   },
   {
    "duration": 19,
    "start_time": "2023-06-02T07:09:50.692Z"
   },
   {
    "duration": 108829,
    "start_time": "2023-06-02T07:09:50.713Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-02T07:11:39.547Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-02T07:11:39.548Z"
   },
   {
    "duration": 1,
    "start_time": "2023-06-02T07:11:39.550Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-02T07:11:39.552Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-02T07:11:39.554Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-02T07:11:39.555Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-02T07:11:39.556Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-02T07:11:39.558Z"
   },
   {
    "duration": 2208,
    "start_time": "2023-06-02T07:12:05.173Z"
   },
   {
    "duration": 2183,
    "start_time": "2023-06-02T07:12:07.384Z"
   },
   {
    "duration": 2055,
    "start_time": "2023-06-02T07:12:09.570Z"
   },
   {
    "duration": 2737,
    "start_time": "2023-06-02T07:12:11.628Z"
   },
   {
    "duration": 1867,
    "start_time": "2023-06-02T07:12:14.368Z"
   },
   {
    "duration": 235,
    "start_time": "2023-06-02T07:12:16.237Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-02T07:12:16.474Z"
   },
   {
    "duration": 2124,
    "start_time": "2023-06-02T07:12:16.483Z"
   },
   {
    "duration": 40,
    "start_time": "2023-06-02T07:12:18.609Z"
   },
   {
    "duration": 30,
    "start_time": "2023-06-02T07:12:18.653Z"
   },
   {
    "duration": 12,
    "start_time": "2023-06-02T07:12:18.685Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-02T07:12:18.699Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-02T07:12:18.705Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-02T07:12:18.716Z"
   },
   {
    "duration": 74308,
    "start_time": "2023-06-02T07:12:18.729Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-02T07:13:33.038Z"
   },
   {
    "duration": 12,
    "start_time": "2023-06-02T07:13:33.048Z"
   },
   {
    "duration": 40,
    "start_time": "2023-06-02T07:13:33.062Z"
   },
   {
    "duration": 56,
    "start_time": "2023-06-02T07:13:33.103Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-02T07:13:33.161Z"
   },
   {
    "duration": 26,
    "start_time": "2023-06-02T07:13:33.167Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-02T07:13:33.197Z"
   },
   {
    "duration": 677277,
    "start_time": "2023-06-02T07:13:33.203Z"
   },
   {
    "duration": 16650,
    "start_time": "2023-06-02T20:39:37.227Z"
   },
   {
    "duration": 79,
    "start_time": "2023-06-02T20:40:14.254Z"
   },
   {
    "duration": 22,
    "start_time": "2023-06-02T20:40:14.526Z"
   },
   {
    "duration": 5120,
    "start_time": "2023-06-02T20:40:14.858Z"
   },
   {
    "duration": 143,
    "start_time": "2023-06-02T20:40:19.983Z"
   },
   {
    "duration": 12,
    "start_time": "2023-06-02T20:40:20.131Z"
   },
   {
    "duration": 26,
    "start_time": "2023-06-02T20:40:20.148Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
